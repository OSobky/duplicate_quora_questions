
TRAIN[steps=100] loss=0.684502 acc=0.609 P=0.609 R=1.000 F1=0.757282
DEV[steps=100] loss=0.010853 acc=0.483 P=0.483 R=1.000 F1=0.651175 *
TRAIN[steps=200] loss=0.690837 acc=0.547 P=0.547 R=1.000 F1=0.707071
DEV[steps=200] loss=0.010836 acc=0.483 P=0.483 R=1.000 F1=0.651175
TRAIN[steps=300] loss=0.699025 acc=0.312 P=0.312 R=1.000 F1=0.476190
DEV[steps=300] loss=0.010824 acc=0.483 P=0.483 R=1.000 F1=0.651175
TRAIN[steps=400] loss=0.693869 acc=0.438 P=0.438 R=1.000 F1=0.608696
DEV[steps=400] loss=0.010817 acc=0.483 P=0.483 R=1.000 F1=0.651175
TRAIN[steps=500] loss=0.692821 acc=0.547 P=0.000 R=0.000 F1=0.000000
DEV[steps=500] loss=0.010812 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=600] loss=0.691425 acc=0.562 P=0.000 R=0.000 F1=0.000000
DEV[steps=600] loss=0.010808 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=700] loss=0.690962 acc=0.562 P=0.000 R=0.000 F1=0.000000
DEV[steps=700] loss=0.010807 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=800] loss=0.691944 acc=0.531 P=0.000 R=0.000 F1=0.000000
DEV[steps=800] loss=0.010806 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=900] loss=0.691153 acc=0.547 P=0.000 R=0.000 F1=0.000000
DEV[steps=900] loss=0.010806 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1000] loss=0.690499 acc=0.562 P=0.000 R=0.000 F1=0.000000
DEV[steps=1000] loss=0.010806 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1100] loss=0.687052 acc=0.609 P=0.000 R=0.000 F1=0.000000
DEV[steps=1100] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1200] loss=0.687101 acc=0.609 P=0.000 R=0.000 F1=0.000000
DEV[steps=1200] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1300] loss=0.698243 acc=0.422 P=0.000 R=0.000 F1=0.000000
DEV[steps=1300] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1400] loss=0.689077 acc=0.578 P=0.000 R=0.000 F1=0.000000
DEV[steps=1400] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1500] loss=0.687058 acc=0.609 P=0.000 R=0.000 F1=0.000000
DEV[steps=1500] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1600] loss=0.697785 acc=0.438 P=0.000 R=0.000 F1=0.000000
DEV[steps=1600] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1700] loss=0.692660 acc=0.516 P=0.000 R=0.000 F1=0.000000
DEV[steps=1700] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1800] loss=0.688629 acc=0.578 P=0.000 R=0.000 F1=0.000000
DEV[steps=1800] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=1900] loss=0.697276 acc=0.453 P=0.000 R=0.000 F1=0.000000
DEV[steps=1900] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=2000] loss=0.703491 acc=0.359 P=0.000 R=0.000 F1=0.000000
DEV[steps=2000] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
TRAIN[steps=2100] loss=0.695648 acc=0.469 P=0.000 R=0.000 F1=0.000000
DEV[steps=2100] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
No improvement for a long time, early-stopping at best F1=0.651175320148468
tensor([1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 0.,
        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 1., 0.,
        0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0.,
        1., 1., 1., 1., 0., 0., 0., 0., 0., 1.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=100] loss=0.688167 acc=0.578 P=0.000 R=0.000 F1=0.000000
tensor([0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0.,
        0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 1., 0., 1., 0.,
        0., 0., 1., 1., 1., 1., 1., 0., 1., 0.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=100] loss=0.691566 acc=0.531 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=100] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 1., 0.,
        1., 1., 1., 1., 0., 1., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0.,
        0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 1., 1.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=200] loss=0.693872 acc=0.500 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=200] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 0.,
        1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1.,
        0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 0., 1.,
        1., 1., 1., 0., 0., 1., 0., 1., 0., 0.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=300] loss=0.690176 acc=0.547 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=300] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.,
        0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0.,
        1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 0.,
        0., 1., 0., 0., 0., 0., 0., 0., 1., 0.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=400] loss=0.690273 acc=0.547 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=400] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([1., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 1.,
        0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0.,
        0., 1., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 0., 1., 1., 0., 0.,
        1., 1., 1., 1., 1., 1., 0., 0., 1., 0.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=500] loss=0.691497 acc=0.531 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=500] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0., 1., 1., 1., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 1.,
        0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1.,
        0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1.,
        0., 0., 1., 0., 0., 0., 1., 1., 0., 1.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=600] loss=0.695701 acc=0.469 P=0.000 R=0.000 F1=0.000000
tensor([0., 0., 0.,  ..., 0., 1., 1.]) tensor([False, False, False,  ..., False, False, False])
DEV[steps=600] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
tensor([1., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,
        1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,
        0., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 1.,
        1., 0., 0., 1., 1., 0., 1., 1., 0., 1.]) tensor([False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False, False, False, False, False, False, False,
        False, False, False, False])
TRAIN[steps=700] loss=0.687308 acc=0.594 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.4841260612010956 0.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.4841257631778717 1.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412570357322693 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412612080574036 1.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.48412615060806274 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.4841260015964508 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.4841260015964508 1.0
0.48412588238716125 1.0
0.48412588238716125 1.0
0.48412594199180603 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412516713142395 0.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412570357322693 0.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412588238716125 0.0
0.4841260015964508 0.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.4841260015964508 1.0
0.4841260015964508 0.0
0.48412567377090454 1.0
0.48412588238716125 1.0
0.48412588238716125 0.0
0.48412588238716125 1.0
0.4841257631778717 1.0
0.48412570357322693 1.0
0.4841260015964508 1.0
TRAIN[steps=100] loss=0.693651 acc=0.500 P=0.000 R=0.000 F1=0.000000
DEV[steps=100] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4841788113117218 1.0
0.4841788113117218 0.0
0.484178751707077 0.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.484178751707077 1.0
0.4841788709163666 1.0
0.4841788113117218 0.0
0.4841788113117218 0.0
0.4841788113117218 0.0
0.4841788709163666 1.0
0.48417869210243225 0.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788709163666 0.0
0.484178751707077 1.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.48417893052101135 0.0
0.4841788113117218 0.0
0.4841788709163666 1.0
0.4841788113117218 0.0
0.48417899012565613 1.0
0.4841788113117218 1.0
0.4841788709163666 1.0
0.4841788709163666 0.0
0.48417893052101135 1.0
0.4841788709163666 0.0
0.4841788113117218 1.0
0.484178751707077 0.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788709163666 0.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.48417893052101135 0.0
0.4841788113117218 0.0
0.4841788113117218 1.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.484178751707077 1.0
0.4841788113117218 1.0
0.48417943716049194 1.0
0.4841788113117218 0.0
0.4841788113117218 1.0
0.4841788709163666 1.0
0.4841788113117218 0.0
0.4841788113117218 1.0
0.4841788709163666 0.0
0.4841788113117218 0.0
0.4841786324977875 1.0
0.4841788709163666 0.0
0.4841788113117218 0.0
0.4841788113117218 1.0
0.4841788113117218 0.0
0.4841788113117218 0.0
0.4841788113117218 0.0
0.4841788113117218 0.0
TRAIN[steps=200] loss=0.694637 acc=0.484 P=0.000 R=0.000 F1=0.000000
DEV[steps=200] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841008484363556 0.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841008484363556 0.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841008484363556 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841008484363556 1.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841005802154541 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.48410096764564514 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841008484363556 0.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841008484363556 0.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.48410090804100037 1.0
0.4841008484363556 0.0
0.4841008484363556 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.4841007888317108 0.0
0.4841007888317108 1.0
0.48410096764564514 0.0
0.48410090804100037 1.0
0.4841007888317108 0.0
0.4841005802154541 1.0
0.4841007888317108 0.0
0.48410072922706604 1.0
0.4841007888317108 0.0
0.4841008484363556 0.0
0.48410096764564514 1.0
0.4841007888317108 0.0
0.4841007888317108 0.0
0.4841007888317108 0.0
TRAIN[steps=300] loss=0.684707 acc=0.641 P=0.000 R=0.000 F1=0.000000
DEV[steps=300] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.48379629850387573 1.0
0.4837964177131653 0.0
0.48379629850387573 0.0
0.4837964177131653 0.0
0.48379597067832947 1.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837961792945862 0.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837963581085205 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.48379629850387573 0.0
0.48379653692245483 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.48379653692245483 0.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.48379653692245483 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837964177131653 0.0
0.4837966859340668 0.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 1.0
0.4837964177131653 0.0
0.4837964177131653 1.0
0.4837964177131653 1.0
TRAIN[steps=400] loss=0.691646 acc=0.531 P=0.000 R=0.000 F1=0.000000
DEV[steps=400] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.48214036226272583 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.48214003443717957 0.0
0.48214009404182434 0.0
0.48214003443717957 0.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.48214027285575867 1.0
0.48214003443717957 1.0
0.48214027285575867 1.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.48213991522789 0.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 1.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 0.0
0.4821401536464691 1.0
0.4821401536464691 0.0
TRAIN[steps=500] loss=0.687085 acc=0.594 P=0.000 R=0.000 F1=0.000000
DEV[steps=500] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959635734558 0.0
0.4824959933757782 1.0
0.4824959635734558 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.48249557614326477 0.0
0.4824959933757782 1.0
0.4824959635734558 1.0
0.4824959933757782 0.0
0.4824959635734558 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.48249611258506775 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.48249611258506775 0.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.48249611258506775 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 1.0
0.4824954569339752 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959635734558 1.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.4824959933757782 0.0
0.48249611258506775 1.0
0.4824959933757782 0.0
0.4824959933757782 0.0
TRAIN[steps=600] loss=0.691571 acc=0.531 P=0.000 R=0.000 F1=0.000000
DEV[steps=600] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 1.0
0.482898086309433 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289814591407776 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289814591407776 1.0
0.48289820551872253 1.0
0.4828982353210449 0.0
0.48289820551872253 1.0
0.482898086309433 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289820551872253 1.0
0.48289820551872253 0.0
0.48289796710014343 1.0
0.48289820551872253 1.0
0.48289820551872253 1.0
0.48289820551872253 0.0
TRAIN[steps=700] loss=0.688386 acc=0.578 P=0.000 R=0.000 F1=0.000000
DEV[steps=700] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.48212409019470215 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.4821239709854126 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.4821239709854126 0.0
0.48212385177612305 0.0
0.4821239709854126 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.4821241497993469 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.4821239113807678 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212429881095886 1.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 1.0
0.4821241796016693 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
0.48212409019470215 0.0
0.48212409019470215 1.0
0.48212409019470215 0.0
TRAIN[steps=800] loss=0.689316 acc=0.562 P=0.000 R=0.000 F1=0.000000
DEV[steps=800] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270586133003235 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.482705682516098 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.4827055037021637 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.4827054440975189 1.0
0.48270556330680847 0.0
0.4827054440975189 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.482705682516098 0.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.4827054440975189 0.0
0.48270556330680847 0.0
0.48270562291145325 0.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 0.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 1.0
0.48270556330680847 0.0
TRAIN[steps=900] loss=0.692664 acc=0.516 P=0.000 R=0.000 F1=0.000000
DEV[steps=900] loss=0.010805 acc=0.517 P=0.000 R=0.000 F1=0.000000
model sim and label tuples:
0.4810009300708771 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.481000691652298 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810006618499756 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.481000691652298 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.4810008108615875 1.0
0.48100075125694275 0.0
0.4810009300708771 0.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.4810008108615875 1.0
0.48100101947784424 1.0
0.4810006022453308 0.0
0.4810008108615875 1.0
0.4810008108615875 0.0
0.4810008108615875 0.0
0.48100101947784424 0.0
0.4810008108615875 0.0
