{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework (Mini-project)\n",
    "\n",
    "Objective of this assignment is to implement the basic building blocks of a Deep Learning pipeline on a sample supervised-learning problem in **PyTorch**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Name: Omar Elsobky\n",
    "\n",
    "Matriculation No.: 03737994 \n",
    "\n",
    "**Important:** Do not forget to fill the places where you see `### Your code goes here ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "______________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task and Setup\n",
    "\n",
    "In this assignment, we want you to experience doing a mini-project in PyTorch. You are supposed to build the different parts of the pipeline as illustrated in the course notebooks (DataLoading, Model, Loss, Training, Evaluation). For this purpose, we will use a dataset from Quora containing question pairs and labels whether the pair is a duplicate or not.\n",
    "\n",
    "The data can be found in Moodle in a csv file. The data is This data is subject to Quora's [Terms of Service](https://www.quora.com/about/tos), allowing for non-commercial use. The dataset was downloaded from https://www.kaggle.com/quora/question-pairs-dataset. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Classifiying Duplicate Question Pairs** \n",
    "\n",
    "\n",
    "We want to build a DL Model that can predict whether two questions from a Quora dataset are duplicates or not. Note that the two questions must not be identical as you will see in the dataset, rather they semantically mean almost the same thing.\n",
    "\n",
    "To make the setup a bit simpler, we extracted and prepared a small subset of the original data consisting of 50k examples. Additionally, we removed questions that are too long or too short, we kept questions of length between 30 and 50 characters. Those 50k examples should serve as training and validation data, please consider making a reasonable split. Do not train on the validation data, just use it to evaluate your model.\n",
    "\n",
    "**Model Inputs and Label**:\n",
    "\n",
    "Input Format: 2 questions, for each question you will have an input of `BATCH_SIZE X SEQ_LEN`, where SEQ_LEN is the number of tokens in the question. Of course, if you will stack the input into batches, you will need to pad the questions (i.e. add a padding token or zeros at the end of the question to make all questions equal in length).\n",
    "\n",
    "Label Format: `BATCH_SIZE X 1`, please note that the extra dimension (`X 1`) is optional and dependent on your implementaiton, you could have a simple 1D tensor of length `BATCH_SIZE`, where each value is either 0 or 1 indicating that the two questions are either non-duplicates or not, respectively.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. Please read the csv file and explore the dataset a bit in order to familize yourself with the problem before working on it.\n",
    "\n",
    "2. In your custom DataLoader you have to make sure that you provide two questions for each example, this should be done in the `def __getitem__(self, idx)` method.\n",
    "\n",
    "3. Please work at the word-level, your tokens are words. You will need to preprocess the data accordingly. Feel free to write simple Python code that can do the job, but also consider using tokenizers, stemmers, and lemmatizers from known NLP libraries such as [NLTK](https://www.nltk.org/) or [SpaCy](https://spacy.io/).\n",
    "\n",
    "4. You will need to encode the words into integers to be able to pass them to the model, you will also need to keep track of the vocabulary. For this purpose, you can also write your own Python code or use an out-of-the-box module such as `torchtext.data.vocab` (see example in the data loading notebook). You can include this part in your Dataset class if you like.\n",
    "\n",
    "5. You will most probalby need to use an embedding layer as input to the model, it will then take the sequence of integers and return numeric vectors representing each word. Please consider using pre-trained embeddings, there are multiple ways how to load these into your newly-created Embedding layer, `torchtext` also provides some easy ways to do that.\n",
    "\n",
    "6. With Embeddings, you will have two options:\n",
    "    - train your own embeddings on the task, either by starting from random weights or after loading pre-trained embeddings (this will take more time and probably need Colab or GPU)\n",
    "    - or freeze the pre-trained embeddings and train the rest of the network (make sure the embedding layer is frozen, `requires_grad` is set to `False`.\n",
    "\n",
    "7. Note that you will need to encode the questions as integers based on the vocabulary you are using. This sequence of integers will be fed as input to the model (embeddings lookup, then the following layers).\n",
    "\n",
    "8. Take care that the model have two inputs (two sentences in parallel). This should be done in your implementation of the `def forward(self, question1, qustion2)` in your custom model class.\n",
    "\n",
    "9. Since you need to feed both question to your model, in the `forward` you will have to let each question go through a couple of layers to get a representation for each question. Then, you will have to combine the two representations in any way you see possible (e.g. multiply them, subtract them, concatenate them). Finally, with this final representation, you will have to let it go through a couple of layers (mostly fully-connected layers) and then predict the outcome (2 classes).\n",
    "\n",
    "10. This is generally a binary classification problem, you can use a classification loss to train your model. There are more advanced loss functions that are related to Siamese Networks (which is this architecture since it has multiple parallel inputs), feel free to use or explore them.\n",
    "\n",
    "11. A nice lecture about the topic is here: Siamese Networks and Similarity Learning Lecture, Prof. Dr. Laura Leal-Taix√©, Advanced Deep Learning for Computer Vision Course:https://www.youtube.com/watch?v=6e65XfwmIWE\n",
    "\n",
    "12. Good summary and course notes of Deep Learning specialization on Coursera: https://github.com/mbadry1/DeepLearning.ai-Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libs below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import exists, join\n",
    "from os import mkdir\n",
    "import glob\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Data Loading (30 Points)\n",
    "\n",
    "1. Write code to read the dataset after you download it from Moodle.\n",
    "2. Explore some examples and check if you need to do some data cleaning or remove some bad examples.\n",
    "3. Decide on what preprocessing steps you will do to the text of the questions.\n",
    "4. Build a custom PyTorch dataset where you implement the required methods `__getitem__` and `__len__`. Do not forget to integrate any preprocessing steps in the class. Make sure that you also have a function that applies the whole preprocessing to a raw example, this will be very helpful when you want to predict for test examples later.\n",
    "5. Split the data into train and validation data. Use a reasonable split ratio.\n",
    "6. Create PyTorch dataloaders for train and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.\\\\data',\n",
       " '.\\\\GoogleNews-vectors-negative300.bin.gz',\n",
       " '.\\\\Homework-WS21.ipynb',\n",
       " '.\\\\mini_quora_dataset_30_50_50k.csv',\n",
       " '.\\\\models']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Your code goes here ####\n",
    "##checking the files in this location\n",
    "glob.glob(join(\"./\", \"*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>max_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307549</td>\n",
       "      <td>603786</td>\n",
       "      <td>603787</td>\n",
       "      <td>What are some different ways to make money fast?</td>\n",
       "      <td>What are fast ways to make money?</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221916</td>\n",
       "      <td>437426</td>\n",
       "      <td>437427</td>\n",
       "      <td>How can I continue to improve my English?</td>\n",
       "      <td>How can I understand english?</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177830</td>\n",
       "      <td>351280</td>\n",
       "      <td>351281</td>\n",
       "      <td>How do I promote my youtube videos?</td>\n",
       "      <td>What is the best way to promote YouTube videos?</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128018</td>\n",
       "      <td>253605</td>\n",
       "      <td>253606</td>\n",
       "      <td>How can I organize a Quora Meetup in Pune?</td>\n",
       "      <td>Is there a Pune Quora Meetup group?</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177761</td>\n",
       "      <td>351144</td>\n",
       "      <td>351145</td>\n",
       "      <td>What is the most badass moment of Game of Thro...</td>\n",
       "      <td>Who will die in season 5 of Game of Thrones?</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>95213</td>\n",
       "      <td>189005</td>\n",
       "      <td>189006</td>\n",
       "      <td>How does drop shipping work exactly?</td>\n",
       "      <td>What is drop shipping and how does it work?</td>\n",
       "      <td>1</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>62682</td>\n",
       "      <td>124658</td>\n",
       "      <td>124659</td>\n",
       "      <td>What are the best movies to watch in Hollywood?</td>\n",
       "      <td>Which are the best Hollywood movies of all time?</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>109939</td>\n",
       "      <td>218008</td>\n",
       "      <td>218009</td>\n",
       "      <td>Am I a sociopath, schizoid, or neither?</td>\n",
       "      <td>Am I a sociopath?</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>370854</td>\n",
       "      <td>725712</td>\n",
       "      <td>725713</td>\n",
       "      <td>What is your marketing strategy?</td>\n",
       "      <td>What is a market strategy?</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>268327</td>\n",
       "      <td>527813</td>\n",
       "      <td>527814</td>\n",
       "      <td>What will be the qualifying marks for neet 2016?</td>\n",
       "      <td>What might be the qualifing mark in neet 2016?</td>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    qid1    qid2  \\\n",
       "0      307549  603786  603787   \n",
       "1      221916  437426  437427   \n",
       "2      177830  351280  351281   \n",
       "3      128018  253605  253606   \n",
       "4      177761  351144  351145   \n",
       "...       ...     ...     ...   \n",
       "49995   95213  189005  189006   \n",
       "49996   62682  124658  124659   \n",
       "49997  109939  218008  218009   \n",
       "49998  370854  725712  725713   \n",
       "49999  268327  527813  527814   \n",
       "\n",
       "                                               question1  \\\n",
       "0       What are some different ways to make money fast?   \n",
       "1              How can I continue to improve my English?   \n",
       "2                    How do I promote my youtube videos?   \n",
       "3             How can I organize a Quora Meetup in Pune?   \n",
       "4      What is the most badass moment of Game of Thro...   \n",
       "...                                                  ...   \n",
       "49995               How does drop shipping work exactly?   \n",
       "49996    What are the best movies to watch in Hollywood?   \n",
       "49997            Am I a sociopath, schizoid, or neither?   \n",
       "49998                   What is your marketing strategy?   \n",
       "49999   What will be the qualifying marks for neet 2016?   \n",
       "\n",
       "                                              question2  is_duplicate  \\\n",
       "0                     What are fast ways to make money?             1   \n",
       "1                         How can I understand english?             1   \n",
       "2       What is the best way to promote YouTube videos?             1   \n",
       "3                   Is there a Pune Quora Meetup group?             0   \n",
       "4          Who will die in season 5 of Game of Thrones?             0   \n",
       "...                                                 ...           ...   \n",
       "49995       What is drop shipping and how does it work?             1   \n",
       "49996  Which are the best Hollywood movies of all time?             1   \n",
       "49997                                 Am I a sociopath?             0   \n",
       "49998                        What is a market strategy?             0   \n",
       "49999    What might be the qualifing mark in neet 2016?             1   \n",
       "\n",
       "       max_length  \n",
       "0              48  \n",
       "1              41  \n",
       "2              47  \n",
       "3              42  \n",
       "4              50  \n",
       "...           ...  \n",
       "49995          43  \n",
       "49996          48  \n",
       "49997          39  \n",
       "49998          32  \n",
       "49999          48  \n",
       "\n",
       "[50000 rows x 7 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Reading the CSV file\n",
    "questions_df = pd.read_csv(\"mini_quora_dataset_30_50_50k.csv\", sep=\",\")\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                        201871\n",
       "qid1                                      398348\n",
       "qid2                                      398349\n",
       "question1       How can I create an Android app?\n",
       "question2                                    NaN\n",
       "is_duplicate                                   0\n",
       "max_length                                    32\n",
       "Name: 12405, dtype: object"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_df.iloc[12405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307549</td>\n",
       "      <td>603786</td>\n",
       "      <td>603787</td>\n",
       "      <td>What are some different ways to make money fast?</td>\n",
       "      <td>What are fast ways to make money?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221916</td>\n",
       "      <td>437426</td>\n",
       "      <td>437427</td>\n",
       "      <td>How can I continue to improve my English?</td>\n",
       "      <td>How can I understand english?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177830</td>\n",
       "      <td>351280</td>\n",
       "      <td>351281</td>\n",
       "      <td>How do I promote my youtube videos?</td>\n",
       "      <td>What is the best way to promote YouTube videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128018</td>\n",
       "      <td>253605</td>\n",
       "      <td>253606</td>\n",
       "      <td>How can I organize a Quora Meetup in Pune?</td>\n",
       "      <td>Is there a Pune Quora Meetup group?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177761</td>\n",
       "      <td>351144</td>\n",
       "      <td>351145</td>\n",
       "      <td>What is the most badass moment of Game of Thro...</td>\n",
       "      <td>Who will die in season 5 of Game of Thrones?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>95213</td>\n",
       "      <td>189005</td>\n",
       "      <td>189006</td>\n",
       "      <td>How does drop shipping work exactly?</td>\n",
       "      <td>What is drop shipping and how does it work?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>62682</td>\n",
       "      <td>124658</td>\n",
       "      <td>124659</td>\n",
       "      <td>What are the best movies to watch in Hollywood?</td>\n",
       "      <td>Which are the best Hollywood movies of all time?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>109939</td>\n",
       "      <td>218008</td>\n",
       "      <td>218009</td>\n",
       "      <td>Am I a sociopath, schizoid, or neither?</td>\n",
       "      <td>Am I a sociopath?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>370854</td>\n",
       "      <td>725712</td>\n",
       "      <td>725713</td>\n",
       "      <td>What is your marketing strategy?</td>\n",
       "      <td>What is a market strategy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>268327</td>\n",
       "      <td>527813</td>\n",
       "      <td>527814</td>\n",
       "      <td>What will be the qualifying marks for neet 2016?</td>\n",
       "      <td>What might be the qualifing mark in neet 2016?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    qid1    qid2  \\\n",
       "0      307549  603786  603787   \n",
       "1      221916  437426  437427   \n",
       "2      177830  351280  351281   \n",
       "3      128018  253605  253606   \n",
       "4      177761  351144  351145   \n",
       "...       ...     ...     ...   \n",
       "49995   95213  189005  189006   \n",
       "49996   62682  124658  124659   \n",
       "49997  109939  218008  218009   \n",
       "49998  370854  725712  725713   \n",
       "49999  268327  527813  527814   \n",
       "\n",
       "                                               question1  \\\n",
       "0       What are some different ways to make money fast?   \n",
       "1              How can I continue to improve my English?   \n",
       "2                    How do I promote my youtube videos?   \n",
       "3             How can I organize a Quora Meetup in Pune?   \n",
       "4      What is the most badass moment of Game of Thro...   \n",
       "...                                                  ...   \n",
       "49995               How does drop shipping work exactly?   \n",
       "49996    What are the best movies to watch in Hollywood?   \n",
       "49997            Am I a sociopath, schizoid, or neither?   \n",
       "49998                   What is your marketing strategy?   \n",
       "49999   What will be the qualifying marks for neet 2016?   \n",
       "\n",
       "                                              question2  is_duplicate  \n",
       "0                     What are fast ways to make money?             1  \n",
       "1                         How can I understand english?             1  \n",
       "2       What is the best way to promote YouTube videos?             1  \n",
       "3                   Is there a Pune Quora Meetup group?             0  \n",
       "4          Who will die in season 5 of Game of Thrones?             0  \n",
       "...                                                 ...           ...  \n",
       "49995       What is drop shipping and how does it work?             1  \n",
       "49996  Which are the best Hollywood movies of all time?             1  \n",
       "49997                                 Am I a sociopath?             0  \n",
       "49998                        What is a market strategy?             0  \n",
       "49999    What might be the qualifing mark in neet 2016?             1  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Removing the max_length column, Because i think it was added to filter the data and when i checked i did not find this column in the original data \n",
    "\n",
    "questions_df = questions_df.drop(labels='max_length', axis='columns')\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "###checking the balance of the data \n",
    "\n",
    "questions_df.is_duplicate.value_counts().plot(kind=\"bar\")\n",
    "\n",
    "## From the results the data set is quiet balanced "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>307549</td>\n",
       "      <td>603786</td>\n",
       "      <td>603787</td>\n",
       "      <td>What are some different ways to make money fast?</td>\n",
       "      <td>What are fast ways to make money?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>221916</td>\n",
       "      <td>437426</td>\n",
       "      <td>437427</td>\n",
       "      <td>How can I continue to improve my English?</td>\n",
       "      <td>How can I understand english?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>177830</td>\n",
       "      <td>351280</td>\n",
       "      <td>351281</td>\n",
       "      <td>How do I promote my youtube videos?</td>\n",
       "      <td>What is the best way to promote YouTube videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>128018</td>\n",
       "      <td>253605</td>\n",
       "      <td>253606</td>\n",
       "      <td>How can I organize a Quora Meetup in Pune?</td>\n",
       "      <td>Is there a Pune Quora Meetup group?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>177761</td>\n",
       "      <td>351144</td>\n",
       "      <td>351145</td>\n",
       "      <td>What is the most badass moment of Game of Thro...</td>\n",
       "      <td>Who will die in season 5 of Game of Thrones?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>95213</td>\n",
       "      <td>189005</td>\n",
       "      <td>189006</td>\n",
       "      <td>How does drop shipping work exactly?</td>\n",
       "      <td>What is drop shipping and how does it work?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>62682</td>\n",
       "      <td>124658</td>\n",
       "      <td>124659</td>\n",
       "      <td>What are the best movies to watch in Hollywood?</td>\n",
       "      <td>Which are the best Hollywood movies of all time?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>109939</td>\n",
       "      <td>218008</td>\n",
       "      <td>218009</td>\n",
       "      <td>Am I a sociopath, schizoid, or neither?</td>\n",
       "      <td>Am I a sociopath?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>370854</td>\n",
       "      <td>725712</td>\n",
       "      <td>725713</td>\n",
       "      <td>What is your marketing strategy?</td>\n",
       "      <td>What is a market strategy?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>268327</td>\n",
       "      <td>527813</td>\n",
       "      <td>527814</td>\n",
       "      <td>What will be the qualifying marks for neet 2016?</td>\n",
       "      <td>What might be the qualifing mark in neet 2016?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows √ó 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           id    qid1    qid2  \\\n",
       "0      307549  603786  603787   \n",
       "1      221916  437426  437427   \n",
       "2      177830  351280  351281   \n",
       "3      128018  253605  253606   \n",
       "4      177761  351144  351145   \n",
       "...       ...     ...     ...   \n",
       "49995   95213  189005  189006   \n",
       "49996   62682  124658  124659   \n",
       "49997  109939  218008  218009   \n",
       "49998  370854  725712  725713   \n",
       "49999  268327  527813  527814   \n",
       "\n",
       "                                               question1  \\\n",
       "0       What are some different ways to make money fast?   \n",
       "1              How can I continue to improve my English?   \n",
       "2                    How do I promote my youtube videos?   \n",
       "3             How can I organize a Quora Meetup in Pune?   \n",
       "4      What is the most badass moment of Game of Thro...   \n",
       "...                                                  ...   \n",
       "49995               How does drop shipping work exactly?   \n",
       "49996    What are the best movies to watch in Hollywood?   \n",
       "49997            Am I a sociopath, schizoid, or neither?   \n",
       "49998                   What is your marketing strategy?   \n",
       "49999   What will be the qualifying marks for neet 2016?   \n",
       "\n",
       "                                              question2  label  \n",
       "0                     What are fast ways to make money?      1  \n",
       "1                         How can I understand english?      1  \n",
       "2       What is the best way to promote YouTube videos?      1  \n",
       "3                   Is there a Pune Quora Meetup group?      0  \n",
       "4          Who will die in season 5 of Game of Thrones?      0  \n",
       "...                                                 ...    ...  \n",
       "49995       What is drop shipping and how does it work?      1  \n",
       "49996  Which are the best Hollywood movies of all time?      1  \n",
       "49997                                 Am I a sociopath?      0  \n",
       "49998                        What is a market strategy?      0  \n",
       "49999    What might be the qualifing mark in neet 2016?      1  \n",
       "\n",
       "[50000 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### renaming the is_duplicate column to label\n",
    "\n",
    "questions_df.rename(columns={\"is_duplicate\": \"label\"}, inplace=True)\n",
    "questions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "# ! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format(\n",
    "            'GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hend El\n",
      "[nltk_data]     Sobky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Hend El\n",
      "[nltk_data]     Sobky\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#### NOOW doing the rest if the data preprocessing in the dataLoader class!\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "class QuestionsDataset(Dataset):\n",
    "    \"\"\" Questions Dataset.\"\"\"\n",
    "\n",
    "    #define the constructor of this dataset object\n",
    "    def __init__(self, questions_df):\n",
    "        \"\"\" \n",
    "\n",
    "        Args: \n",
    "            questions_df(pd.DataFarme): DataFrame object contains the two questions and the label \n",
    "        \"\"\"\n",
    "        self.questions_df = questions_df\n",
    "        self.zero_counter = 0\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.clean_data()\n",
    "        self.prepare_data()\n",
    "        \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.q1_vec_tensor.shape[0]\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        return self.q1_vec_tensor[idx], self.q2_vec_tensor[idx], self.label[idx], \n",
    "        # self.questions_df[\"question1\"][idx], self.questions_df[\"question2\"][idx]\n",
    "\n",
    "    def clean_data(self):\n",
    "        ### retriving the max length of all questions so i can get the embeddings of of all questions with the same seq size\n",
    "        # self.seq_len = self.questions_df[\"max_length\"].max()\n",
    "        # print(self.seq_len)\n",
    "        self.questions_df = self.questions_df.drop(labels=['max_length','id', 'qid1', 'qid2'], axis='columns')\n",
    "        self.questions_df.rename(columns={\"is_duplicate\": \"label\"}, inplace=True)\n",
    "        # display(self.questions_df.head())\n",
    "\n",
    "        #### Checking if there was any null values in any of the features\n",
    "        print(self.questions_df.isna().sum())\n",
    "        # print(self.questions_df.shape)\n",
    "        display(self.questions_df[self.questions_df.isnull().any(axis=1)])\n",
    "\n",
    "        ### After Checking, thre are two rows where question2 is NaN. We will keep only the rows with no NaN values \n",
    "        self.questions_df.dropna(inplace=True)\n",
    "        print(self.questions_df.shape)\n",
    "        print(self.questions_df.isna().sum())\n",
    "\n",
    "        print(self.questions_df.iloc[12405])\n",
    "        display(self.questions_df.head())\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "    def prepare_data(self):\n",
    "        ### Here we need to tokonize the sentnces and then get the embeddings of each work in the senctnce. \n",
    "        ### I will be creating a function called sent2vec which take the whole sentence and create a vector for the whole sentance \n",
    "        ### After checking the data a little bit I found the largest seq number is 11 so i will padd evey q to have 11 seq length\n",
    "        # self.seq_len = 0\n",
    "        print(\"in preapare data \",self.questions_df.shape)\n",
    "        q1_vec = []\n",
    "        q2_vec = []\n",
    "        for q1, q2 in zip(self.questions_df[\"question1\"], self.questions_df[\"question2\"]):\n",
    "            q1_vec.append(self.question2vec(q1))\n",
    "            q2_vec.append(self.question2vec(q2))\n",
    "            \n",
    "\n",
    "        print(self.zero_counter)\n",
    "            \n",
    "\n",
    "\n",
    "        # print(\"seq length: \", self.seq_len)\n",
    "\n",
    "        self.q1_vec_tensor = torch.tensor(np.array(q1_vec))\n",
    "        self.q2_vec_tensor = torch.tensor(np.array(q2_vec))\n",
    "        self.label = torch.tensor(np.array(self.questions_df[\"label\"]))\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"q1 shape:\", self.q1_vec_tensor.shape)\n",
    "        # print(self.q1_vec_tensor)\n",
    "        # print(\"------------------------------------\")\n",
    "        \n",
    "        print(\"q2 shape:\", self.q2_vec_tensor.shape)\n",
    "        print(\"labels shape:\", self.label.shape )\n",
    "        # print(\"seq length: \", self.seq_len)\n",
    "        # self.questions_df[\"q1_tokens\"] = q1_tokens\n",
    "        # self.questions_df[\"q2_tokens\"] = q2_tokens\n",
    "        # print(q1_tokens)\n",
    "\n",
    "    \n",
    "    ### this function is inspired after reading about word2vec and this artical https://hub.packtpub.com/use-tensorflow-and-nlp-to-detect-duplicate-quora-questions-tutorial/ \n",
    "    ### I will be using pretrained Word2vec model GoogleNews-vectors-negative (which kinda transferlearning for the embeddings)\n",
    "    ### I will be using question_pair2vec to retrive the embeddings of both question with the same seq size \n",
    "    def question2vec(self, q):\n",
    "        \n",
    "        M = []\n",
    "        X = []\n",
    "        # print(q)\n",
    "        words = nltk.word_tokenize(q)\n",
    "        for w in words:\n",
    "            if w not in stop_words:\n",
    "                if w.isalpha():\n",
    "                    if w in self.word2vec_model:\n",
    "                        M.append(self.word2vec_model[w])\n",
    "                        X.append(w)\n",
    "        M = np.array(M)\n",
    "        if len(M) > 0:\n",
    "            if len(M) < 11:\n",
    "                padding_size = 11 - len(M)\n",
    "                for _ in range(padding_size):\n",
    "                    M = np.append(M ,np.zeros((1,300)), axis=0)\n",
    "            # v = M.sum(axis=0)\n",
    "            # print(M)\n",
    "            # self.seq_len = len(M) if (len(M) > self.seq_len) else self.seq_len\n",
    "            # print(self.seq_len)\n",
    "            return M\n",
    "            # return v / np.sqrt((v ** 2).sum())\n",
    "        else:\n",
    "            self.zero_counter+=1\n",
    "            return np.zeros((11,300))\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "       \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question1    0\n",
      "question2    2\n",
      "label        0\n",
      "dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12405</th>\n",
       "      <td>How can I create an Android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41646</th>\n",
       "      <td>How can I develop android app?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              question1 question2  label\n",
       "12405  How can I create an Android app?       NaN      0\n",
       "41646    How can I develop android app?       NaN      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49998, 3)\n",
      "question1    0\n",
      "question2    0\n",
      "label        0\n",
      "dtype: int64\n",
      "question1    How do you define your \"worth\"?\n",
      "question2       How do you define ‚Äòmeaning‚Äô?\n",
      "label                                      0\n",
      "Name: 12406, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are some different ways to make money fast?</td>\n",
       "      <td>What are fast ways to make money?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How can I continue to improve my English?</td>\n",
       "      <td>How can I understand english?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How do I promote my youtube videos?</td>\n",
       "      <td>What is the best way to promote YouTube videos?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I organize a Quora Meetup in Pune?</td>\n",
       "      <td>Is there a Pune Quora Meetup group?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the most badass moment of Game of Thro...</td>\n",
       "      <td>Who will die in season 5 of Game of Thrones?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           question1  \\\n",
       "0   What are some different ways to make money fast?   \n",
       "1          How can I continue to improve my English?   \n",
       "2                How do I promote my youtube videos?   \n",
       "3         How can I organize a Quora Meetup in Pune?   \n",
       "4  What is the most badass moment of Game of Thro...   \n",
       "\n",
       "                                         question2  label  \n",
       "0                What are fast ways to make money?      1  \n",
       "1                    How can I understand english?      1  \n",
       "2  What is the best way to promote YouTube videos?      1  \n",
       "3              Is there a Pune Quora Meetup group?      0  \n",
       "4     Who will die in season 5 of Game of Thrones?      0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in preapare data  (49998, 3)\n",
      "4\n",
      "q1 shape: torch.Size([49998, 11, 300])\n",
      "q2 shape: torch.Size([49998, 11, 300])\n",
      "labels shape: torch.Size([49998])\n"
     ]
    }
   ],
   "source": [
    "## creating instance of questions_Dataset\n",
    "questions_dataset = QuestionsDataset(pd.read_csv(\"mini_quora_dataset_30_50_50k.csv\", sep=\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.1602,  0.2168,  0.0549,  ...,  0.0454, -0.0811,  0.0894],\n",
       "         [-0.0742, -0.0737, -0.0194,  ...,  0.0586,  0.0481, -0.2217],\n",
       "         [ 0.1699,  0.0143, -0.0454,  ...,  0.0415, -0.1001, -0.1885],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[ 0.1602,  0.2168,  0.0549,  ...,  0.0454, -0.0811,  0.0894],\n",
       "         [-0.0742, -0.0737, -0.0194,  ...,  0.0586,  0.0481, -0.2217],\n",
       "         [ 0.1807, -0.1562,  0.2656,  ..., -0.0752,  0.0417, -0.1289],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor(0))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for idx, (data) in enumerate(questions_dataset):\n",
    "#     print(idx)\n",
    "\n",
    "questions_dataset[12405]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0231,  0.1533,  0.1523,  ...,  0.1445,  0.1328, -0.0859],\n",
       "         [ 0.0269, -0.0762,  0.0270,  ..., -0.0165, -0.2197,  0.0757],\n",
       "         [ 0.1523,  0.2949, -0.0009,  ...,  0.1680, -0.0139,  0.1826],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor([[-0.0231,  0.1533,  0.1523,  ...,  0.1445,  0.1328, -0.0859],\n",
       "         [ 0.1445, -0.0124,  0.1191,  ...,  0.1924,  0.0371, -0.1104],\n",
       "         [ 0.1523,  0.2949, -0.0009,  ...,  0.1680, -0.0139,  0.1826],\n",
       "         ...,\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "        dtype=torch.float64),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of the whole dataset:  49998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([10633,  6690, 23430, ..., 25550, 30594, 23360])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Splitting the data\n",
    "\n",
    "# We have a PyTorch Subset object, what if we want to manage the indices for subsetting the original dataset?\n",
    "\n",
    "n_samples = len(questions_dataset)\n",
    "print(\"size of the whole dataset: \", n_samples)\n",
    "\n",
    "# Shuffle indices with np.random.permutation, also fix seed if you want reproducability\n",
    "\n",
    "shuffled_indices = np.random.permutation(n_samples)\n",
    "shuffled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples:  39999\n",
      "val  samples:   9999\n"
     ]
    }
   ],
   "source": [
    "### ratio is 80% and 20%\n",
    "\n",
    "val_ratio = 0.2\n",
    "\n",
    "validationset_inds = shuffled_indices[:int(n_samples * val_ratio)]\n",
    "trainingset_inds = shuffled_indices[int(n_samples * val_ratio):]\n",
    "print(\"train samples: \", len(trainingset_inds))\n",
    "print(\"val  samples:  \", len(validationset_inds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39999, 9999)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = torch.utils.data.Subset(questions_dataset, indices=trainingset_inds)\n",
    "val_dataset = torch.utils.data.Subset(questions_dataset, indices=validationset_inds)\n",
    "len(train_dataset), len(val_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(624, 156)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.save(join(\"./\", \"data\", \"questions\", \"train_idx.npy\"), trainingset_inds)\n",
    "np.save(join(\"./\", \"data\", \"questions\", \"val_idx.npy\"), validationset_inds)\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True, sampler=None,\n",
    "                                           collate_fn=None,\n",
    "                                           drop_last=True\n",
    "                                           )\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=False, sampler=None,\n",
    "                                           collate_fn=None,\n",
    "                                           drop_last=True\n",
    "                                           )\n",
    "\n",
    "# Let us check some properties of data loader object\n",
    "\n",
    "len(train_loader), len(val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n"
     ]
    }
   ],
   "source": [
    "for i,_ in enumerate(train_loader):\n",
    "    # print(q1.shape)\n",
    "    # print(q2.shape)\n",
    "    # print(label)\n",
    "    # print(qes1)\n",
    "    # print(qes2)\n",
    "    # break\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39936, 39999)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of batches X batch_size = should give us the size of the dataset\n",
    "len(train_loader) * batch_size, len(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9984, 9999)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_loader) * batch_size, len(val_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch_size:  64\n",
      "Sampler info:  <torch.utils.data.sampler.RandomSampler object at 0x0000021435F452B0>\n",
      "Dataset is also there:\n",
      " <__main__.QuestionsDataset object at 0x00000213422BF4C0>\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch_size: \", train_loader.batch_size)\n",
    "print(\"Sampler info: \", train_loader.sampler)\n",
    "print(\"Dataset is also there:\\n\", train_loader.dataset.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.keyedvectors.KeyedVectors object at 0x000002134B342580>\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Model (20 Points)\n",
    "\n",
    "1. Explore what possible models for the task could be. You do not need to come up with a very complex model, a relatively small model consisting of the following sequence would be okay: {*Embeddings for the input - LSTM or CNN to process the sequence - Linear Layers to learn features from the combined representation of the questions - Output Layer*} would also be fine, just take care of the sizing of the different layers. Please always check the sizing after each layer and make sure you understand the dimensions correclty and they map to what you have in mind.\n",
    "2. Build a model class.\n",
    "3. Test your model with one batch from your dataloader and check the input and output shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here ####\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Dup_Qestions(nn.Module):\n",
    "    def __init__(self, input_size=300, batch_size=8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        self.q1_block = nn.Sequential(\n",
    "                            torch.nn.Conv1d(11, 30, 3),\n",
    "                            nn.ReLU(),\n",
    "                            torch.nn.Conv1d(30, 20, 3),\n",
    "                            nn.ReLU(),\n",
    "                             torch.nn.Conv1d(20, 10, 3),\n",
    "                            nn.ReLU(),\n",
    "                            torch.nn.Conv1d(10, 1, 3),\n",
    "                            nn.ReLU(),\n",
    "                            )\n",
    "        self.q2_block = nn.Sequential(\n",
    "                            torch.nn.Conv1d(11, 30, 3),\n",
    "                            nn.ReLU(),\n",
    "                            torch.nn.Conv1d(30, 20, 3),\n",
    "                            nn.ReLU(),\n",
    "                             torch.nn.Conv1d(20, 10, 3),\n",
    "                            nn.ReLU(),\n",
    "                            torch.nn.Conv1d(10, 1, 3),\n",
    "                            nn.ReLU(),\n",
    "                            )\n",
    "\n",
    "        \n",
    "        self.classification_block =   nn.Sequential(\n",
    "                nn.Linear(292,120),\n",
    "                nn.Tanh(), \n",
    "                nn.Linear(120,60),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(60,1),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "\n",
    "        # self.hidden2class = nn.Linear(298, 1)\n",
    "        # self.sig = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, q1, q2):\n",
    "\n",
    "\n",
    "        q1_conv = self.q1_block(q1)\n",
    "        q2_conv = self.q2_block(q2)\n",
    "        # print(\"q1_conv size:\",q1_conv.shape)\n",
    "        # print(\"q2_conv size:\",q2_conv.shape)\n",
    "\n",
    "        # sim = F.cosine_similarity(q1_conv.view(batch_size, q1_conv.shape[2]), q2_conv.view(batch_size, q2_conv.shape[2]))\n",
    "        # print(\"sim size:\", sim.shape)\n",
    "\n",
    "        out_sig = self.classification_block(q1_conv.view(batch_size,-1) * q2_conv.view(batch_size,-1))\n",
    "\n",
    "        # out = self.hidden2class(q1_conv * q2_conv)\n",
    "        # print(\"out size: \",out.shape)\n",
    "        # out_sig = self.sig(out)\n",
    "        # print(\"out_sig size: \",out_sig.shape)\n",
    "\n",
    "        return out_sig\n",
    "        # q1_conv, q2_conv, sim\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=0.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, y, y_):  # y, y_ must be same type float (*)\n",
    "        loss = y * torch.pow(1-y_, 2) + (1 - y) * torch.pow(y_-self.margin, 2)\n",
    "        loss = torch.sum(loss) / 2.0 / len(y)   #y.size()[0]\n",
    "        return loss\n",
    "        \n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "output = F.cosine_similarity(input1, input2)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Training (30 Points)\n",
    "\n",
    "1. Develop Training and Validation code.\n",
    "2. Choose a suitable loss function.\n",
    "3. Refactor the code so that it can be easily modified and adapted (use methods, classes, etc...)\n",
    "4. Make sure to save your trained model when you reach a good score on the validation dataset.\n",
    "5. Plot the training and validation losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here ####\n",
    "\n",
    "loss_func = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA\n",
      "q1_block.0.weight True\n",
      "q1_block.0.bias True\n",
      "q1_block.2.weight True\n",
      "q1_block.2.bias True\n",
      "q1_block.4.weight True\n",
      "q1_block.4.bias True\n",
      "q1_block.6.weight True\n",
      "q1_block.6.bias True\n",
      "q2_block.0.weight True\n",
      "q2_block.0.bias True\n",
      "q2_block.2.weight True\n",
      "q2_block.2.bias True\n",
      "q2_block.4.weight True\n",
      "q2_block.4.bias True\n",
      "q2_block.6.weight True\n",
      "q2_block.6.bias True\n",
      "classification_block.0.weight True\n",
      "classification_block.0.bias True\n",
      "classification_block.2.weight True\n",
      "classification_block.2.bias True\n",
      "classification_block.4.weight True\n",
      "classification_block.4.bias True\n"
     ]
    }
   ],
   "source": [
    "model = Dup_Qestions()\n",
    "model\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to(\"cuda\")\n",
    "    print(\"CUDA\")\n",
    "\n",
    "for name, data in model.named_parameters():\n",
    "    print(name, data.requires_grad)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "# optimizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 5, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-5ee2cd5e5156>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mq1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mq2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 5, got 3)"
     ]
    }
   ],
   "source": [
    "total_loss = 0\n",
    "\n",
    "\n",
    "for data in train_loader:\n",
    "    q1, q2, label, _, _ = data\n",
    "    if torch.cuda.is_available():\n",
    "        q1, q2 = q1.to(\"cuda\"), q2.to(\"cuda\"), \n",
    "\n",
    "    print(q1.view(batch_size,1,-1).shape)\n",
    "        \n",
    "    # print(\"Batch shape: \", q1.shape)\n",
    "    # print(\"Labels: \", label)\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # forward pass --> input images go through the model\n",
    "    q1 = q1.float()\n",
    "    q2 = q2.float()\n",
    "    label = label.float()\n",
    "    output = model(q1.view(batch_size,1,-1), q2.view(batch_size,1,-1))\n",
    "    \n",
    "    print(\"Predictions shape: \", output.shape)\n",
    "    \n",
    "    print(output.view(batch_size,1), label.view(batch_size,1))\n",
    "    # compute loss and backward --> compute gradients for all Parameters\n",
    "    loss = loss_func(output.view(batch_size,1), label.view(batch_size,1))\n",
    "    total_loss += loss.item()\n",
    "    loss.backward()\n",
    "    \n",
    "    # optimize --> make a parameter update\n",
    "    optimizer.step()\n",
    "        \n",
    "    break\n",
    "    \n",
    "print('loss: ', total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "def metrics(y, y_pred):\n",
    "    # 8-bit integer (unsigned)\n",
    "    # y, y_pred = torch.ByteTensor(y), torch.ByteTensor(y_pred)\n",
    "    TP = ((y_pred == 1) & (y == 1)).sum().float()\n",
    "    TN = ((y_pred == 0) & (y == 0)).sum().float()\n",
    "    FN = ((y_pred == 0) & (y == 1)).sum().float()\n",
    "    FP = ((y_pred == 1) & (y == 0)).sum().float()\n",
    "    p = TP / (TP + FP).clamp(min=1e-8)\n",
    "    r = TP / (TP + FN).clamp(min=1e-8)\n",
    "    F1 = 2 * r * p / (r + p).clamp(min=1e-8)\n",
    "    acc = (TP + TN) / (TP + TN + FP + FN).clamp(min=1e-8)\n",
    "    return acc, p, r, F1\n",
    "\n",
    "\n",
    "\n",
    "def train(train_iter, dev_iter, model, args):\n",
    "    if args['cuda']:\n",
    "        model.cuda()\n",
    "    # for param in model.parameters():\n",
    "    #     print(param)\n",
    "\n",
    "    def adjust_learning_rate(optimizer, learning_rate, epoch):\n",
    "        lr = learning_rate * (0.1 ** (epoch // 10))\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "        return optimizer\n",
    "\n",
    "    if args['optimizer'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], betas=[0.9, 0.999], eps=1e-8, weight_decay=0)\n",
    "    elif args['optimizer'] == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=args['lr'], momentum=0.9, weight_decay=0.01)\n",
    "    else:\n",
    "        raise Exception('For other optimizers, please add it yourself. supported ones are: SGD and Adam.')\n",
    "\n",
    "    F1_best = 0.0\n",
    "    last_improved_step = 0\n",
    "    model.train()\n",
    "    steps = 0\n",
    "    for epoch in range(1, args['epochs']+1):\n",
    "        for batch in train_iter:\n",
    "            optimizer = adjust_learning_rate(optimizer, args['lr'], epoch)\n",
    "            q1, q2, label = batch\n",
    "            if torch.cuda.is_available():\n",
    "                q1 = q1.to(\"cuda\")\n",
    "                q2 = q2.to(\"cuda\")\n",
    "\n",
    "            q1 = q1.float()\n",
    "            q2 = q2.float()\n",
    "            label = label.float()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            # _, _, score = model(q1, q2)\n",
    "            score = model(q1, q2)\n",
    "\n",
    "            score = score.cpu()\n",
    "\n",
    "            # print('out1', out1.dtype)\n",
    "            # print('target vector', y.dtype)\n",
    "\n",
    "            # loss_function = nn.CrossEntropyLoss()\n",
    "            # loss = loss_function(output, Variable(train_labels))\n",
    "            # criterion = nn.CosineEmbeddingLoss(margin=0, size_average=True, reduce=False)\n",
    "            # loss = criterion(out1, out2, (2 * y - 1))  # cast y to {1, -1} and float type\n",
    "            # criterion = ContrastiveLoss()\n",
    "            # loss = criterion(y, sim)\n",
    "\n",
    "            # loss = F.cross_entropy(sim, y)\n",
    "            # print(score.shape, label.shape)\n",
    "            # loss = F.binary_cross_entropy_with_logits(score, label)\n",
    "            loss = loss_func(score.view(args['batch_size'],1), label.view(args['batch_size'],1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps += 1\n",
    "\n",
    "            if steps % args['log_interval'] == 0:\n",
    "                # _, pred = torch.max(sim.data, 1)\n",
    "                # print('model sim and label tuples:')\n",
    "                # for i, j in zip(score, label):\n",
    "                #     print(i.item(), j.item())\n",
    "\n",
    "                pred = score.data >= 0.4\n",
    "                acc, p, r, f1 = metrics(label, pred)\n",
    "                print('TRAIN[steps={}] loss={:.6f} acc={:.3f} P={:.3f} R={:.3f} F1={:.6f}'.format(\n",
    "                    steps, loss.item(), acc, p, r, f1))\n",
    "            # if steps % args['test_interval'] == 0:\n",
    "            #     loss, acc, p, r, f1 = eval(dev_iter, model)\n",
    "\n",
    "            #     if f1 > F1_best:\n",
    "            #         F1_best = f1\n",
    "            #         last_improved_step = steps\n",
    "            #         if F1_best > 0.5:\n",
    "            #             save_prefix = os.path.join(args['save_dir'], 'snapshot')\n",
    "            #             save_path = '{}_steps{}.pt'.format(save_prefix, steps)\n",
    "            #             torch.save(model, save_path)\n",
    "            #         improved_token = '*'\n",
    "            #     else:\n",
    "            #         improved_token = ''\n",
    "            #     print('DEV[steps={}] loss={:.6f} acc={:.3f} P={:.3f} R={:.3f} F1={:.6f} {}'.format(\n",
    "            #         steps, loss, acc, p, r, f1, improved_token))\n",
    "\n",
    "            # if steps % args['save_interval'] == 0:\n",
    "            #     if not os.path.isdir(args['save_dir']):\n",
    "            #         os.makedirs(args['save_dir'])\n",
    "            #     save_prefix = os.path.join(args['save_dir'], 'snapshot')\n",
    "            #     save_path = '{}_steps{}.pt'.format(save_prefix, steps)\n",
    "            #     torch.save(model, save_path)\n",
    "\n",
    "            if steps - last_improved_step > 2000:  # 2000 steps\n",
    "                print(\"No improvement for a long time, early-stopping at best F1={}\".format(F1_best))\n",
    "                break\n",
    "\n",
    "\n",
    "def eval(data_iter, model):\n",
    "    loss_tot, y_list, y_pred_list = 0, [], []\n",
    "    model.eval()\n",
    "    for x1, x2, y in data_iter:\n",
    "        # if args.cuda:\n",
    "        #     x1, x2, y = Variable(x1).cuda(), Variable(x2).cuda(), Variable(y).cuda()\n",
    "        # else:\n",
    "        #     x1, x2, y = Variable(x1), Variable(x2), Variable(y)\n",
    "        out1, out2, sim = model(x1, x2)\n",
    "        # loss = F.cross_entropy(output, y, size_average=False)\n",
    "        criterion = nn.CosineEmbeddingLoss()\n",
    "        loss = criterion(out1, out2, (2*y-1).float())\n",
    "        loss_tot += loss.item()  # 0-dim scaler\n",
    "        y_pred = sim.data >= 0.5\n",
    "        y_pred_list.append(y_pred)\n",
    "        y_list.append(y)\n",
    "    y_pred = torch.cat(y_pred_list, 0)\n",
    "    y = torch.cat(y_list, 0)\n",
    "    acc, p, r, f1 = metrics(y, y_pred)\n",
    "    size = len(data_iter.dataset)\n",
    "    loss_avg = loss_tot / float(size)\n",
    "    model.train()\n",
    "    return loss_avg, acc, p, r, f1\n",
    "\n",
    "\n",
    "def predict(text, model, text_field, label_feild, cuda_flag):\n",
    "    assert isinstance(text, str)\n",
    "    model.eval()\n",
    "    # text = text_field.tokenize(text)\n",
    "    text = text_field.preprocess(text)\n",
    "    text = [[text_field.vocab.stoi[x] for x in text]]\n",
    "    x = text_field.tensor_type(text)\n",
    "    x = autograd.Variable(x, volatile=True)\n",
    "    if cuda_flag:\n",
    "        x = x.cuda()\n",
    "    print(x)\n",
    "    output = model(x)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    return label_feild.vocab.itos[predicted.data[0][0]+1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN[steps=100] loss=0.577316 acc=0.487 P=0.453 R=0.641 F1=0.530804\n",
      "TRAIN[steps=200] loss=0.541465 acc=0.515 P=0.547 R=0.656 F1=0.596591\n",
      "TRAIN[steps=300] loss=0.572573 acc=0.453 P=0.406 R=0.750 F1=0.527027\n",
      "TRAIN[steps=400] loss=0.652665 acc=0.537 P=0.578 R=0.734 F1=0.646949\n",
      "TRAIN[steps=500] loss=0.610698 acc=0.495 P=0.469 R=0.578 F1=0.517724\n",
      "TRAIN[steps=600] loss=0.539223 acc=0.484 P=0.438 R=0.625 F1=0.514706\n",
      "TRAIN[steps=700] loss=0.633931 acc=0.500 P=0.500 R=0.750 F1=0.600000\n",
      "TRAIN[steps=800] loss=0.531701 acc=0.496 P=0.484 R=0.641 F1=0.551649\n",
      "TRAIN[steps=900] loss=0.544859 acc=0.495 P=0.484 R=0.672 F1=0.562922\n",
      "TRAIN[steps=1000] loss=0.628630 acc=0.527 P=0.562 R=0.719 F1=0.631098\n",
      "TRAIN[steps=1100] loss=0.568973 acc=0.490 P=0.438 R=0.578 F1=0.498077\n",
      "TRAIN[steps=1200] loss=0.584931 acc=0.507 P=0.516 R=0.719 F1=0.600475\n",
      "TRAIN[steps=1300] loss=0.568732 acc=0.503 P=0.516 R=0.609 F1=0.558594\n",
      "TRAIN[steps=1400] loss=0.612763 acc=0.499 P=0.453 R=0.516 F1=0.482359\n",
      "TRAIN[steps=1500] loss=0.541428 acc=0.505 P=0.516 R=0.656 F1=0.577500\n",
      "TRAIN[steps=1600] loss=0.602759 acc=0.506 P=0.516 R=0.688 F1=0.589286\n",
      "TRAIN[steps=1700] loss=0.652899 acc=0.500 P=0.500 R=0.625 F1=0.555556\n",
      "TRAIN[steps=1800] loss=0.479469 acc=0.487 P=0.453 R=0.641 F1=0.530804\n",
      "TRAIN[steps=1900] loss=0.552895 acc=0.516 P=0.531 R=0.750 F1=0.621951\n",
      "TRAIN[steps=2000] loss=0.583712 acc=0.491 P=0.469 R=0.641 F1=0.541373\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n",
      "No improvement for a long time, early-stopping at best F1=0.0\n"
     ]
    }
   ],
   "source": [
    "args = {'lr': 0.0005, \n",
    "        'optimizer': 'Adam', \n",
    "        \"cuda\": True, \n",
    "        'epochs': 100,\n",
    "        'batch_size': batch_size,\n",
    "        'log_interval': 100,\n",
    "        'test_interval': 100,\n",
    "        'save_interval' : 1000,\n",
    "        'save_dir': './models/'\n",
    "        }\n",
    "\n",
    "train(train_loader, val_loader, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DupQesModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, test_loader=None, device='cuda',\n",
    "                 optimizer=None, \n",
    "                 loss_func=torch.nn.CrossEntropyLoss()):\n",
    "        \n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.test_loader = test_loader\n",
    "\n",
    "        self.device = device\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_func = loss_func\n",
    "        \n",
    "        self.epoch = 0\n",
    "        \n",
    "        # if the optimizer is not initialzed \n",
    "        if optimizer:\n",
    "            self.optimizer = optimizer\n",
    "        # else:\n",
    "        #     self.optimizer = torch.optim.Adam(**optimizer_args)\n",
    "                \n",
    "        if self.device == 'cuda':\n",
    "            self.model.cuda()\n",
    "            \n",
    "    def train_epoch(self):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, data in enumerate(train_loader):\n",
    "            q1, q2, label, _, _ = data\n",
    "            if torch.cuda.is_available():\n",
    "                q1, q2 = q1.to(\"cuda\"), q2.to(\"cuda\"), \n",
    "\n",
    "            # print(q1.view(batch_size,1,-1).shape)\n",
    "                \n",
    "            # print(\"Batch shape: \", q1.shape)\n",
    "            # print(\"Labels: \", label)\n",
    "            \n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward pass --> input images go through the model\n",
    "            q1 = q1.float()\n",
    "            q2 = q2.float()\n",
    "            label = label.float()\n",
    "            output = model(q1.view(batch_size,1,-1), q2.view(batch_size,1,-1)).cpu()\n",
    "            \n",
    "            # print(\"Predictions shape: \", output.shape)\n",
    "            \n",
    "            # print(output.view(batch_size,1), label.view(batch_size,1))\n",
    "            # compute loss and backward --> compute gradients for all Parameters\n",
    "            loss = loss_func(output.view(batch_size,1), label.view(batch_size,1))\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            \n",
    "            # optimize --> make a parameter update\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % 100 == 0:\n",
    "                print(total_loss / (i+1))\n",
    "            return (total_loss) / len(train_loader)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        pass\n",
    "    \n",
    "    def train(self, n_epochs):\n",
    "        for e in range(n_epochs):\n",
    "            self.train_epoch()\n",
    "            # self.evaluate()\n",
    "    \n",
    "    def inference(self, loader=None):\n",
    "        \n",
    "        # if loader is none, use self.test_loader\n",
    "            # if self.test_loader is null, break\n",
    "        \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = DupQesModelTrainer(model, train_loader, val_loader, optimizer=optimizer,loss_func=loss_func )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894810795783997\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862773299217224\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948031187057495\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915894150733948\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861934661865234\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872422695159912\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.702491819858551\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893812417984009\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69486403465271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69266676902771\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893554329872131\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904533505439758\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937786340713501\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904413104057312\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893183588981628\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926693916320801\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694918692111969\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7039377689361572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892949938774109\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7016662955284119\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859433650970459\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937893629074097\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926692128181458\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915513277053833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882017254829407\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926689147949219\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971419453620911\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960206031799316\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6815173625946045\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915518045425415\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937881112098694\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690428614616394\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937919855117798\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881752610206604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983017325401306\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983038783073425\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937952041625977\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893000602722168\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7027735114097595\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926690340042114\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937835216522217\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6860008835792542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882277131080627\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871153712272644\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904427409172058\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687091052532196\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881898045539856\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698296070098877\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937985420227051\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858896613121033\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881340742111206\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903932094573975\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693818211555481\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938233971595764\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938275098800659\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972973346710205\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926752924919128\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857330799102783\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926759481430054\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926763653755188\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891831755638123\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938449144363403\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926778554916382\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973653435707092\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973659992218018\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961893439292908\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856698989868164\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695013165473938\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950112581253052\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880156397819519\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6996687650680542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856990456581116\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984913349151611\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903541088104248\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6868759393692017\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926760673522949\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692676305770874\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689189612865448\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950044631958008\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950057744979858\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938409805297852\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6996574997901917\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949970126152039\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961462497711182\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972848176956177\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892326474189758\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903868913650513\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938127279281616\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960838437080383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688137948513031\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938022971153259\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694928765296936\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926702260971069\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937932968139648\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937900185585022\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6982575058937073\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871029734611511\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694889485836029\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904518604278564\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871376633644104\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893492341041565\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937748193740845\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993154883384705\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915611028671265\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992972493171692\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882606148719788\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937664747238159\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904702186584473\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915692090988159\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915697455406189\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697051465511322\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893820762634277\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7014190554618835\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693756103515625\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926651000976562\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959161758422852\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861861944198608\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948205232620239\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969681978225708\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905182600021362\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969443559646606\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001285552978516\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926623582839966\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696881890296936\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874189972877502\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693705677986145\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957826018333435\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936964392662048\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957538723945618\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947116255760193\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947001218795776\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926594972610474\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936684250831604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926591992378235\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6876626014709473\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6836855411529541\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6906622648239136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6866565942764282\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690650224685669\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6946771740913391\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926594972610474\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936745643615723\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6946939826011658\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6967331767082214\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6987670660018921\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6977353096008301\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916486620903015\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696688175201416\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956665515899658\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936559677124023\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936507225036621\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695618748664856\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6838310956954956\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936379671096802\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916816234588623\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696561872959137\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69557785987854\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6945975422859192\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955546736717224\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926589608192444\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888341903686523\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687889575958252\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955202221870422\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002785801887512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936073899269104\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6851068139076233\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936027407646179\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6860625147819519\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6945478320121765\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869888305664062\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6945556402206421\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6974101066589355\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955094337463379\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955059766769409\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954993605613708\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6860541701316833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973767280578613\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964253783226013\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889070868492126\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954697370529175\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689857006072998\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6907932758331299\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694525957107544\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6945242881774902\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6935906410217285\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689873218536377\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6917314529418945\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6917315125465393\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954469680786133\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880186796188354\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963780522346497\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6935891509056091\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6833755373954773\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6907964944839478\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6917243599891663\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889035701751709\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6917155385017395\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888646483421326\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926590800285339\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869005560874939\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916921138763428\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926587820053101\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965768337249756\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6867583990097046\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6946372985839844\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6946462988853455\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926590204238892\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689659595489502\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926591396331787\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916522979736328\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6896284222602844\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916450262069702\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926596760749817\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885703802108765\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690605640411377\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947250962257385\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926605701446533\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916213035583496\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6999616622924805\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6801411509513855\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6895166635513306\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968768835067749\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968907713890076\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873688697814941\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884149312973022\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990609765052795\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687323272228241\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001625895500183\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6840903759002686\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948134899139404\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862013936042786\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937457323074341\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948340535163879\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980992555618286\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689404308795929\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894011497497559\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915752291679382\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915732622146606\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981397867202759\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6839064359664917\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882739067077637\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6970765590667725\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6860387325286865\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959948539733887\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689332902431488\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870922446250916\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915490627288818\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904198527336121\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870198845863342\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938085556030273\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915307641029358\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926736831665039\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892197132110596\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961454153060913\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697316586971283\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857065558433533\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938421130180359\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880058646202087\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903345584869385\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6867989301681519\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692680299282074\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903080940246582\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891070604324341\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890910863876343\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902776956558228\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6842168569564819\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939067840576172\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951404213905334\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988439559936523\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902259588241577\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951661705970764\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951708793640137\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939334869384766\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939339637756348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864959001541138\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6827641725540161\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964330077171326\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951935887336731\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964474320411682\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901963353157043\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901949644088745\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926977038383484\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6989665627479553\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952031850814819\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926972270011902\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914474964141846\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964406371116638\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964329481124878\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902101635932922\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939342021942139\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902188658714294\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939287185668945\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963934898376465\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939222812652588\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926908493041992\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951388120651245\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6841440200805664\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6853682994842529\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691467821598053\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951369047164917\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988112330436707\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951354146003723\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914684772491455\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939079761505127\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939055919647217\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914727687835693\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963275671005249\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951080560684204\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914786696434021\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926853656768799\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698695182800293\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902881264686584\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6986562609672546\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950616240501404\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6867579221725464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903167366981506\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915000677108765\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950365304946899\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903265118598938\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926789283752441\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926787495613098\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961970925331116\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915073394775391\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903398036956787\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691509485244751\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903421878814697\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961811780929565\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7031799554824829\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891887187957764\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949954032897949\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938315629959106\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961328983306885\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6846351623535156\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869384050369263\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915260553359985\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7007138133049011\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6835005879402161\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7018566131591797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938192844390869\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926730871200562\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881065964698792\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858260035514832\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698387622833252\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938155293464661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938146352767944\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691531777381897\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915323138237\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869731545448303\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972385048866272\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915314197540283\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6995214223861694\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915330290794373\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915344595909119\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949446797370911\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915370225906372\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915382146835327\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892745494842529\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858763098716736\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.701751172542572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690403401851654\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847338080406189\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881260871887207\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858314275741577\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892361640930176\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7007374167442322\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6834381818771362\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903558373451233\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961740255355835\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915083527565002\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985446214675903\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6962026953697205\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691504180431366\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891539692878723\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938554644584656\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950335502624512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950331926345825\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950311064720154\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938530206680298\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696195125579834\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880007982254028\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973494291305542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903465390205383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973304748535156\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915157437324524\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7019368410110474\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972835063934326\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697256326675415\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7006402611732483\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937994956970215\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859495639801025\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960110068321228\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959909200668335\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937673687934875\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882894039154053\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872175931930542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883158087730408\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926651000976562\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883198618888855\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959269046783447\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693752110004425\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872332096099854\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937527060508728\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959301233291626\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002795338630676\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861573457717896\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904968023300171\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894124746322632\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948354244232178\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894081234931946\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861431002616882\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981178522109985\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882988810539246\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861006021499634\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69596266746521\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697073221206665\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992828845977783\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882618069648743\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69266676902771\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6860573291778564\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6849393248558044\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993198394775391\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915572881698608\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871047616004944\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926687359809875\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7016268372535706\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915496587753296\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926693320274353\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926692724227905\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993887424468994\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859632730484009\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949045658111572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693786084651947\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960170269012451\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882126331329346\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6982358694076538\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6837790012359619\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960041522979736\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882216930389404\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893301010131836\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960128545761108\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893222332000732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960203647613525\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971388459205627\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960173845291138\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690440833568573\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926681995391846\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.700445830821991\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882386803627014\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926672458648682\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6816343069076538\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981938481330872\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689351499080658\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6849237680435181\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926678419113159\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937811374664307\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870923638343811\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949076652526855\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893036365509033\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915444731712341\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904124021530151\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938042640686035\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938078999519348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983642578125\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881181001663208\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690392255783081\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892461776733398\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926732063293457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903781890869141\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938251256942749\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.686909019947052\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915183067321777\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689193606376648\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950077414512634\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880032420158386\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915056109428406\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879706382751465\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891331672668457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891170144081116\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890996098518372\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6794713735580444\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.684207558631897\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877976655960083\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889878511428833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889554858207703\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.682631254196167\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888874769210815\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927074790000916\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914142966270447\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900994777679443\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927182674407959\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7020114660263062\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980538368225098\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6967321634292603\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927252411842346\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.683354377746582\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900420784950256\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6846498250961304\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.685969889163971\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6818605661392212\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954736709594727\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927400827407837\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955066919326782\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69413161277771\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955276131629944\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6774227023124695\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7011469602584839\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955569386482239\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899431347846985\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885324120521545\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941649317741394\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.684281587600708\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688508152961731\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694180965423584\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927614808082581\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870509386062622\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884682178497314\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956409215927124\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956490278244019\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956541538238525\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913268566131592\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985470056533813\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913267374038696\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927697658538818\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6970958709716797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6841273903846741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884470582008362\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913275122642517\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942132711410522\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884386539459229\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6898800730705261\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7000095844268799\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985607147216797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869900822639465\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869916319847107\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913255453109741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942183971405029\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.684088945388794\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695672333240509\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913228631019592\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7000353932380676\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927738785743713\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6898727416992188\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956732869148254\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7000164985656738\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6783154606819153\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69566410779953\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.686987042427063\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956661939620972\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7043495178222656\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697102427482605\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6999673843383789\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941995620727539\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884787082672119\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884896755218506\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687076210975647\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856608986854553\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899189352989197\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927598714828491\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.681382417678833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941877007484436\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927632093429565\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956260204315186\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7013566493988037\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6999167203903198\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956166625022888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941837072372437\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7041101455688477\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6998153328895569\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983662843704224\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692747950553894\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7052175402641296\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954890489578247\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900079846382141\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913772821426392\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6886898279190063\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6846866607666016\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695397675037384\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940557360649109\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847531795501709\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900683641433716\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7006749510765076\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695364773273468\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913999319076538\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874552369117737\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927170753479004\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927165389060974\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914047002792358\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927157044410706\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900967359542847\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940241456031799\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992558836936951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979358196258545\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862061023712158\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6836162805557251\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914121508598328\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979173421859741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953131556510925\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6849187612533569\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862142086029053\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953169107437134\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927132606506348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992355585098267\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901067495346069\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979250907897949\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979166269302368\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7030920386314392\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991660594940186\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888560056686401\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888716816902161\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697797954082489\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914340257644653\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889106035232544\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914396286010742\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688926100730896\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6826476454734802\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889247298240662\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889175772666931\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939653158187866\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6863676905632019\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990578770637512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901581883430481\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901546716690063\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978146433830261\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901490688323975\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901468634605408\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7003930807113647\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965469121932983\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927052736282349\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927046775817871\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939788460731506\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691430926322937\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6850780844688416\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939747333526611\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927032470703125\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6850736737251282\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7003464698791504\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939770579338074\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.705427348613739\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939696669578552\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964892148971558\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926990151405334\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914454102516174\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902005672454834\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697674036026001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902140378952026\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939300298690796\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690226137638092\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6766942143440247\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988580822944641\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691459059715271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877565979957581\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877493858337402\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926939487457275\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6852436661720276\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926959753036499\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901959180831909\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864238381385803\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914393901824951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990315914154053\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939712762832642\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6838066577911377\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901537179946899\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901458501815796\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927073001861572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914197206497192\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991747617721558\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.683648407459259\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940101981163025\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966177225112915\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822879314422607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822551488876343\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691402792930603\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953603625297546\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692720890045166\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.703359067440033\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980432868003845\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993674039840698\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691395103931427\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927202343940735\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900772452354431\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913992762565613\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900817155838013\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690082311630249\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940367221832275\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874462366104126\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913995742797852\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887578368186951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887519955635071\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927209496498108\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887375116348267\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993803381919861\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6967204809188843\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695387065410614\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6967144012451172\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7033466696739197\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940426826477051\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914008855819702\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914037466049194\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861712336540222\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992499232292175\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901063919067383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862081289291382\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940126419067383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966099143028259\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888198852539062\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901186108589172\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978948712348938\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888277530670166\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6797747015953064\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914144158363342\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914123296737671\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6796855330467224\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992610692977905\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900907754898071\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887672543525696\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953625082969666\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69404536485672\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953758597373962\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913936138153076\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692722499370575\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7020390033721924\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953803300857544\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940478086471558\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993389129638672\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688761293888092\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927176713943481\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940297484397888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979555487632751\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953250765800476\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953136324882507\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940059065818787\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.701738715171814\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7016832232475281\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.685066819190979\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990333199501038\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6876654028892517\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939506530761719\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889543533325195\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951817870140076\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6840217709541321\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939310431480408\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939287185668945\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902252435684204\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7050164937973022\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902360916137695\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939135193824768\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914693713188171\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69877028465271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6878410577774048\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6854352951049805\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914780735969543\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938939094543457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951004266738892\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7011264562606812\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914829015731812\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6866916418075562\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926836371421814\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6974680423736572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6855217218399048\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914893984794617\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914892792701721\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950708627700806\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914893388748169\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891028881072998\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926830410957336\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6962682604789734\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914885640144348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938773393630981\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6855206489562988\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696269154548645\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950743794441223\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950730085372925\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6986504197120667\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950626373291016\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879354119300842\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938652992248535\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695044994354248\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903209686279297\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697389543056488\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6844591498374939\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950263977050781\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985418200492859\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973564624786377\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938422918319702\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69383704662323\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694988489151001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6846118569374084\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6823232173919678\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984342336654663\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938260793685913\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892216801643372\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857665777206421\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938287019729614\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915193796157837\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984645128250122\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903603076934814\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938338279724121\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915174126625061\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938341856002808\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938340663909912\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880444288253784\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903582811355591\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880349516868591\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938396692276001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6845207214355469\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926777958869934\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692678689956665\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938564777374268\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891405582427979\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6997801065444946\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687944769859314\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6843782067298889\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689110279083252\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695074737071991\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890856623649597\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695093035697937\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6854434609413147\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938992738723755\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6866073608398438\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914678812026978\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963711977005005\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951522827148438\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6815998554229736\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6852666139602661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877161264419556\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6826772093772888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.680076003074646\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927042007446289\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991392970085144\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692710280418396\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697918713092804\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953269839286804\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900960803031921\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691403329372406\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7072049379348755\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953494548797607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914029717445374\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861536502838135\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900904774665833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979731917381287\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6808964014053345\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927176117897034\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874452233314514\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913978457450867\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7033214569091797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874213218688965\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687416672706604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913935542106628\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913918852806091\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927238702774048\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927245855331421\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873756647109985\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954071521759033\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887006163597107\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900385618209839\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7008150815963745\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900334358215332\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927295327186584\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873326301574707\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994863748550415\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873270273208618\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981384754180908\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6886768341064453\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927305459976196\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994894742965698\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954297423362732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913818120956421\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927281618118286\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940705180168152\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927265524864197\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913871765136719\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980716586112976\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847235560417175\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927234530448914\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940549612045288\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940531134605408\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927220821380615\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691394567489624\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900695562362671\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927208304405212\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7086172699928284\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874410510063171\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927176713943481\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914035677909851\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940261125564575\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927147507667542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6848840117454529\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979327201843262\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6848958134651184\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874998211860657\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927139163017273\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992480754852295\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874900460243225\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7005555629730225\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901035904884338\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927136182785034\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953198909759521\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927127838134766\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6823130249977112\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6875073909759521\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888015866279602\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927147507667542\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966467499732971\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822222471237183\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953494548797607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6795310974121094\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.701995849609375\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927218437194824\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.699373722076416\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847407221794128\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980538368225098\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993885040283203\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887292861938477\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940534710884094\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980410814285278\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874130964279175\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69537353515625\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887472867965698\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993398666381836\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940408945083618\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900813579559326\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687451183795929\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940340995788574\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927174925804138\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914024353027344\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887743473052979\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697976291179657\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992847919464111\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966472864151001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927143335342407\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992266774177551\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888195276260376\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862430572509766\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927095055580139\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888387799263\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991567611694336\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69142085313797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952778100967407\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6875763535499573\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6798903346061707\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952760815620422\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6849911212921143\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978679895401001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965836882591248\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965844631195068\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991620659828186\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927081346511841\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978462934494019\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952670812606812\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6850489377975464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952517628669739\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927031874656677\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914336681365967\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965018510818481\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889099478721619\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939622163772583\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964793801307678\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926989555358887\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952057480812073\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901962161064148\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864568591117859\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001811265945435\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6989189982414246\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864932775497437\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889798641204834\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6865087151527405\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914557814598083\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001277804374695\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988813877105713\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697629988193512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.682853639125824\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877769231796265\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6865465044975281\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6853050589561462\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902225017547607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902149319648743\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877184510231018\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901973485946655\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002314329147339\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69521564245224\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7015179395675659\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939575672149658\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696468710899353\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695205569267273\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6851946711540222\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690197765827179\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692696750164032\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889492869377136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926970481872559\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951984167098999\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6826942563056946\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964569091796875\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002232670783997\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864334940910339\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926980018615723\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964596509933472\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6814211010932922\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952095627784729\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6989835500717163\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691442608833313\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69269859790802\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939536929130554\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926981806755066\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939513683319092\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6876891255378723\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6989595890045166\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697699248790741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914489269256592\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939401030540466\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889694929122925\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988952159881592\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926933526992798\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6976270079612732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877748370170593\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951436996459961\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695136547088623\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6853715181350708\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6975623965263367\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963353157043457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902629733085632\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914770603179932\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963082551956177\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950939297676086\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6878816485404968\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890875101089478\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950803995132446\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.701062023639679\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938756108283997\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879263520240784\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879347562789917\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903090476989746\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926812529563904\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903085708618164\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6867457628250122\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696250855922699\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6962542533874512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692682147026062\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6974420547485352\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691493570804596\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891213655471802\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938676834106445\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891238689422607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950536966323853\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879380941390991\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926813721656799\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6974323391914368\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914945244789124\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914948225021362\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689122200012207\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879322528839111\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698630154132843\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938717365264893\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914925575256348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7010070085525513\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950543522834778\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6997795104980469\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891461610794067\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691504716873169\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7055578231811523\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693840503692627\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961464881896973\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926742196083069\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881009936332703\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949489116668701\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904042959213257\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938005685806274\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892921924591064\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881750226020813\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926698088645935\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971594095230103\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915490627288818\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870757341384888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915501952171326\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949087977409363\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915497779846191\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893100738525391\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937904357910156\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688183069229126\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960409879684448\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847993731498718\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692670464515686\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926708817481995\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858739852905273\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915346384048462\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949560642242432\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938180327415466\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915265917778015\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903749704360962\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6995885372161865\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857593655586243\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938300132751465\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6868919730186462\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903557777404785\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984957456588745\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938421130180359\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961749792098999\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961734890937805\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6833663582801819\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7019992470741272\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926766037940979\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938381195068359\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880366802215576\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984732747077942\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938326358795166\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892108917236328\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949819326400757\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903710961341858\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938249468803406\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.678882360458374\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693827211856842\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892096996307373\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949925422668457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6996398568153381\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984770894050598\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938334107398987\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892091631889343\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6869044899940491\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857461929321289\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6787832975387573\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985008120536804\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915085315704346\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961989998817444\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926791071891785\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903228759765625\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914989948272705\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903129816055298\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926814913749695\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6962514519691467\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6998288035392761\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914921998977661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950600147247314\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938689351081848\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6986090540885925\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950440406799316\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856129169464111\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903266906738281\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698556661605835\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961972117424011\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891697645187378\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950117349624634\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7008278965950012\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696153461933136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892149448394775\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938229203224182\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694962739944458\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858319640159607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6995006799697876\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870017051696777\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971994638442993\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983146071434021\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693793773651123\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6982641220092773\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960062980651855\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693773090839386\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7003603577613831\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937558650970459\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905001401901245\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894375085830688\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980159878730774\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937268376350403\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6958362460136414\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884564161300659\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947547197341919\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6832842826843262\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947426199913025\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936999559402466\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885115504264832\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978459358215332\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947304010391235\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905959844589233\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916297078132629\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916308403015137\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926601529121399\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.686491847038269\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936900019645691\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926603317260742\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687501072883606\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916258931159973\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916232109069824\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957821249961853\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947447061538696\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978731155395508\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916199326515198\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695780873298645\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936987042427063\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936963200569153\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874936819076538\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695759654045105\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936920881271362\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6906003355979919\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6875142455101013\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916298866271973\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905967593193054\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947282552719116\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885209083557129\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6936980485916138\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916216611862183\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692660927772522\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690575361251831\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957967281341553\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6895231008529663\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6895173192024231\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874073147773743\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937172412872314\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689485490322113\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905361413955688\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6958666443824768\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948050856590271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894451379776001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915882229804993\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937426924705505\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969869136810303\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.685097336769104\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926646828651428\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861452460289001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883004307746887\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948599815368652\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6838576197624207\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904517412185669\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698242723941803\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949073076248169\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688181459903717\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937963962554932\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960592269897461\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696064829826355\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881445050239563\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904044151306152\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960790753364563\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972185373306274\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7028946876525879\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971977949142456\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892912983894348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960372924804688\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687079131603241\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6848586201667786\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691552460193634\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.703840434551239\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915543675422668\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960029006004333\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893430948257446\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926674842834473\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6970844268798828\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959700584411621\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871811151504517\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915707588195801\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948535442352295\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937575340270996\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915757060050964\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6850489377975464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893978118896484\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937567591667175\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69594407081604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861081719398499\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7036190032958984\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698133647441864\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937555074691772\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915784478187561\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904973387718201\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948279738426208\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915849447250366\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937413215637207\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980398893356323\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905206441879272\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862513422966003\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862527132034302\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001564502716064\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693732738494873\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862526535987854\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948024034500122\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969426870346069\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948001980781555\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979955434799194\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947880983352661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979555487632751\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691608726978302\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968543529510498\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874490976333618\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968168616294861\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6978327035903931\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6813479661941528\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690605878829956\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6854733228683472\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693689227104187\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6875070929527283\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.693694531917572\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696807324886322\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6895482540130615\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874642372131348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6864036321640015\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6989514827728271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979164481163025\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690558910369873\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937141418457031\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905558109283447\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979332566261292\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937152743339539\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6863477230072021\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873940229415894\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884358525276184\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873587965965271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6947956681251526\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948046088218689\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915898323059082\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894335150718689\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894222497940063\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980910897254944\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937525272369385\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883092522621155\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695941150188446\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871994137763977\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937625408172607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687172532081604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6992835402488708\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915630102157593\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981947422027588\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6970866918563843\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893587112426758\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893610954284668\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904624700546265\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926668882369995\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871466040611267\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915606260299683\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904488801956177\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926681399345398\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948994398117065\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926687955856323\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870763897895813\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881827116012573\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904181241989136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971933245658875\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926712989807129\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972131729125977\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994866728782654\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949390769004822\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926710605621338\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6802533268928528\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904093027114868\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7006044387817383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6983352899551392\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6881493330001831\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.699448823928833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6836567521095276\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870352029800415\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971872448921204\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904118061065674\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960622072219849\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904105544090271\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7017154693603516\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994375586509705\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915467381477356\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937881708145142\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904385685920715\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882197260856628\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893352270126343\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937789916992188\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6971103549003601\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959938406944275\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871386170387268\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871424913406372\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893489956855774\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904511451721191\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948897242546082\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937806606292725\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960085034370422\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926683187484741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926683187484741\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915557980537415\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882187724113464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960098743438721\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904405951499939\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904390454292297\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904363632202148\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870779395103455\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937914490699768\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6904205083847046\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926705241203308\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926709413528442\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949383020401001\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7028858661651611\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892729997634888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870117783546448\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926712989807129\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926713585853577\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6892678737640381\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69608074426651\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6790318489074707\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903896331787109\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915269494056702\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972802877426147\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961376070976257\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926751732826233\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903620362281799\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926757097244263\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949954032897949\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926759481430054\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915155053138733\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903537511825562\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950013637542725\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.69151371717453\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689186692237854\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880173683166504\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973484754562378\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985213160514832\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903420686721802\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973468661308289\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973390579223633\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891897320747375\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880346536636353\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.684555172920227\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903514862060547\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7019996643066406\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915121078491211\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938414573669434\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891849637031555\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6868547797203064\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915104985237122\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926777362823486\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938491463661194\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6950234174728394\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926785707473755\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698544442653656\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961925029754639\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926776170730591\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985077857971191\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891908645629883\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857215762138367\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973106861114502\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822633743286133\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6891999840736389\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6961596012115479\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926763653755188\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903501749038696\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6868531107902527\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6821658611297607\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698549747467041\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985686421394348\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926798224449158\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938601732254028\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903186440467834\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.685589075088501\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6879405975341797\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6843556761741638\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6878993511199951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6830586791038513\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695112943649292\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963508725166321\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963693499565125\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939218044281006\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926924586296082\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926931738853455\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6803129315376282\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688963770866394\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6839466691017151\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7027650475502014\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939629912376404\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964983940124512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6838304400444031\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939745545387268\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888803839683533\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6863129138946533\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927069425582886\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888432502746582\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6849484443664551\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966136693954468\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953246593475342\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6835532188415527\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822007298469543\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940425634384155\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7020276188850403\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927235126495361\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7007367610931396\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887182593345642\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940615177154541\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6953989863395691\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940613985061646\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6847118139266968\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913882493972778\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691387414932251\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900469660758972\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900436878204346\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994475722312927\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927276849746704\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6886958479881287\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6833134889602661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859877109527588\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6886718273162842\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6845824718475342\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.702293872833252\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899996995925903\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968563795089722\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7023614645004272\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872451901435852\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954874992370605\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872453093528748\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858676671981812\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968716382980347\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954970359802246\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968750953674316\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7037541270256042\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927387714385986\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6858959794044495\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859049201011658\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927363276481628\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6968348026275635\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954652070999146\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6995477080345154\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872991323471069\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6859511137008667\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900203227996826\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7035834789276123\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694084882736206\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6981318593025208\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7021499872207642\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873696446418762\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.698061466217041\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993638277053833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940402984619141\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914030909538269\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927146315574646\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6966182589530945\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888245344161987\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888371706008911\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888459324836731\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965639591217041\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927065849304199\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901463270187378\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914274096488953\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952574253082275\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952524185180664\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939746141433716\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965072751045227\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6825863718986511\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952282190322876\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914382576942444\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927001476287842\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6863992214202881\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002658843994141\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926996111869812\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7065343856811523\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6876896023750305\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6839585900306702\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889538168907166\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939442157745361\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964390873908997\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687711238861084\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926959753036499\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695188581943512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914502382278442\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001653909683228\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914527416229248\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001332640647888\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6877513527870178\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963908672332764\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890036463737488\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6804196834564209\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951490640640259\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.682854413986206\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902259588241577\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914563179016113\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.697658360004425\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889666318893433\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951866149902344\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6889551877975464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6876993775367737\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926977634429932\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6851621270179749\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952222585678101\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964952349662781\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952351927757263\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965049505233765\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.688900351524353\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952378749847412\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6800265908241272\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901610493659973\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888800263404846\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991011500358582\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.682457685470581\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927075386047363\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7017375826835632\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901277899742126\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927096843719482\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965899467468262\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6952961683273315\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901256442070007\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914178729057312\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940011382102966\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862539052963257\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914173364639282\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6797720193862915\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927118301391602\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6822881102561951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900956034660339\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6808667778968811\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6887426376342773\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6967318058013916\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954132914543152\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6832895278930664\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873077750205994\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6900069117546082\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927387714385986\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6872240304946899\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899705529212952\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927480101585388\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913496851921082\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694161057472229\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.691343367099762\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941750645637512\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884969472885132\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7027379870414734\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6998932361602783\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689911425113678\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.685638427734375\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7027432322502136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941853165626526\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984490156173706\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899213194847107\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927574872970581\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984105110168457\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6842971444129944\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6857131719589233\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871198415756226\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899341940879822\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6955808401107788\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687102198600769\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695589005947113\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7026763558387756\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941720843315125\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885174512863159\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927555203437805\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.68429034948349\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969913244247437\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941670775413513\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941659450531006\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6885258555412292\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6800699830055237\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856933236122131\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913408637046814\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6899170875549316\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6856328248977661\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6870383024215698\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927676200866699\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6812201142311096\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6840634346008301\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957008838653564\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927824020385742\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913107633590698\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957507729530334\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6853636503219604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6808644533157349\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7017936706542969\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6988165378570557\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.701840877532959\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7033454775810242\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6943035125732422\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882994174957275\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6912999153137207\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942924857139587\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002643346786499\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942830681800842\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6898200511932373\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6913071870803833\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6824307441711426\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927880644798279\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972265243530273\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6898311972618103\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883550882339478\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001743316650391\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7016396522521973\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942552924156189\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6957158446311951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927792429924011\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6942347884178162\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6956807971000671\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6985636353492737\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689886212348938\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6812757849693298\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7013736963272095\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6970558166503906\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6998900175094604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6984350681304932\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692755401134491\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969656348228455\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7067175507545471\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6941304206848145\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927402019500732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6954689025878906\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6873050332069397\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927299499511719\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927275061607361\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7007490396499634\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927224397659302\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6993312239646912\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861481666564941\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6874833703041077\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7005373239517212\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6940096020698547\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6927096843719482\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888474822044373\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6888577342033386\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914253234863281\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6965413689613342\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990830302238464\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914320588111877\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6901678442955017\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964932680130005\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6863988637924194\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939578652381897\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6964683532714844\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926978826522827\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6976993083953857\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7001738548278809\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6902140378952026\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914577484130859\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6963851451873779\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926905512809753\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6951325535774231\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6939047574996948\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914753913879395\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6914786100387573\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890732049942017\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.686674952507019\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6854732036590576\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6890729665756226\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926859617233276\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938962936401367\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6987450122833252\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6975321173667908\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938959360122681\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926860213279724\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926854252815247\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6962948441505432\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7046873569488525\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6998452544212341\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6986082196235657\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6973831057548523\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6868477463722229\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6903584599494934\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6938273906707764\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6995560526847839\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949515342712402\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6960670948028564\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926698684692383\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937854886054993\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948872804641724\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948720812797546\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915706396102905\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883081197738647\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959205269813538\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.687262237071991\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905081868171692\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6862053871154785\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905093193054199\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937429308891296\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915845274925232\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894214153289795\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.692664623260498\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.689409613609314\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926652193069458\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7024790644645691\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926652193069458\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.695929229259491\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6980940103530884\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894187927246094\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905053853988647\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6851183176040649\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948227286338806\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6883456707000732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6991530656814575\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6937451362609863\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905048489570618\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694823145866394\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915856003761292\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.699131429195404\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6948140263557434\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926633715629578\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894569993019104\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915958523750305\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905310153961182\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915972232818604\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6990571618080139\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6969165802001953\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894826889038086\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916043162345886\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6894932985305786\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.696885883808136\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6852827668190002\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6979377865791321\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6916077733039856\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.694769024848938\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6884526610374451\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6895040273666382\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.682121992111206\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6905441284179688\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.686276912689209\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926633715629578\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926640868186951\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6959140300750732\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.7002739906311035\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6861386299133301\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6926653981208801\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871994137763977\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6893739104270935\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915645599365234\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6871323585510254\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6882157921791077\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.690429151058197\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6814000010490417\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6994944214820862\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6949604749679565\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6915249824523926\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6972904205322266\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880481839179993\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n",
      "0.6880326867103577\n",
      "q1_conv size: torch.Size([64, 1, 292])\n",
      "q2_conv size: torch.Size([64, 1, 292])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-176-3fafb216f243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-173-dcba315175df>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, n_epochs)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[1;31m# self.evaluate()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-173-dcba315175df>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m             \u001b[1;31m# optimize --> make a parameter update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\sgd.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    134\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'momentum_buffer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m             F.sgd(params_with_grad,\n\u001b[0m\u001b[0;32m    137\u001b[0m                   \u001b[0md_p_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m                   \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36msgd\u001b[1;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov)\u001b[0m\n\u001b[0;32m    171\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m                 \u001b[0mbuf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(n_epochs=50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Evaluation (20 Points)\n",
    "\n",
    "1. Report some suitable evaluation metrics. If you stick to standard classification, please report the classification metrics we discussed in the evaluation notebook.\n",
    "2. Check some example and results from the training data.\n",
    "3. Check some examples and results from the validation data (not used for training).\n",
    "4. Come up with one pair of questions and see if your model can produce a reasonable prediction for them. For this, you need to apply the preprocessing pipeline and encoding on the questions' text and make inference to see if the model predicts that they are duplicates.\n",
    "5. Conclude with some comments\n",
    "6. Give us your feedback about the task (at least a sentence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code goes here ####\n",
    "\n",
    "\n",
    "### do not forget to add your comments and feedback ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Submission Notes:**\n",
    "\n",
    "1. You can surely use external files to organize your code in classes or modules (e.g. `.py` files). However, please make sure that the notebook can run without errors and all the required files are attached in your submission (e.g. .zip file). If everything is confined in one single notebook, just submit the notebook.\n",
    "2. If you use special libraries or packages, please indicate that clearly and add a `requirements.txt` file to your submission.\n",
    "3. Do not upload the dataset nor submit it in any way.\n",
    "4. Please do not copy code from someone else, the idea here is that you get a chance to write some code in PyTorch and solve the problem on your own. On the other hand, discuss with your colleagues and support them as much as needed. \n",
    "5. You can of course reuse the code in all the notebooks of the course.\n",
    "6. You can also use code and/or ideas from the internet (e.g. Kaggle notebooks), but please always do the following:\n",
    "    - make sure you understand the code so that you can use it correctly\n",
    "    - cite the source in your notebook as markdown or as a comment in the code (`# adopted from .......`) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation Criteria:**\n",
    "\n",
    "The most important idea we will use for evaluation is that you should get every component of the complete pipeline (1) doing what it is supposed to do and (2) fitting with the other components.\n",
    "\n",
    "Concrete Examples are:\n",
    "1. Your code runs and we can reproduce the results.\n",
    "2. Dataset: Your code reads the dataset, preprocesses it, splits the text into tokens, etc...\n",
    "3. Dataloading: Your dataset and dataloader produce correct inputs and labels (two questions as input, one binary label).\n",
    "4. Model: Your model takes the input (processes each question correctly) and produces a binary prediction (or score).\n",
    "5. Loss: your chosen loss is suitable for the task (e.g. binary cross entropy in case of the simple classification setup).\n",
    "6. Training: your code for training and validation works without errors and the loss on the training and validation data decreases over the course of training. You do not have to achieve a specific performance.\n",
    "7. Evaluation: you discuss the results and pick correctly some questions from train and validation and show the respective predictions (preferably showing the original question text and not the encoded integer sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:**\n",
    "If you do any of the following ideas, you get a bonus, additionally you get to learn more, which is better than the bonus:\n",
    "\n",
    "1. Build a baseline model to compare and benchmark your deep learning model against. You can in this case use a classical machine learning model from `scikit-learn`. A starter example is here: https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "2. Use W&B or Tensorboard to visualize your training. TensorBoard tutorial is available here: (https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html)\n",
    "3. Achieve a relatively good performance on the task.\n",
    "4. Use a learning rate scheduler to improve training. (https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)\n",
    "5. Use pre-trained embeddings correctly.\n",
    "6. Use an advanced loss suitable for a siamese-network.\n",
    "7. Implement any relevant new idea or discuss an interesting insight about the task."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c97ae72702803e9b81d07acd7bfd28776dddc76d0f0b98d4123a375eb7af355"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
